# CIFAR-10 specific configuration
inherit: base_config.yaml

# Device configuration
device: 0  # Use first GPU (cuda:0)

dataset:
  name: cifar10
  num_classes: 10
  input_size: [32, 32]
  mean: [0.4914, 0.4822, 0.4465]
  std: [0.2023, 0.1994, 0.2010]

model:
  architecture: resnet18
  pretrained_path: models/resnet18_cifar10_best.pth 

# Paths
plot_dir: plots/cifar10
log_dir: logs/cifar10

# Training parameters
num_epochs: 30
batch_size: 128
learning_rate: 0.001
target_coverage: 0.9

# Training configuration
training:
  grad_clip:
    enabled: true
    max_norm: 0.5
  loss_weights:
    coverage: 2.0  # Increased from 1.0 to prioritize coverage
    size: 1.0      # lambda2
    margin: 0.2    # margin loss weight

# Optimizer configuration
optimizer:
  name: 'AdamW'
  params:
    lr: 0.001
    weight_decay: 0.01
  scheduler:
    name: 'OneCycleLR'
    params:
      max_lr: 0.001
      pct_start: 0.2
      div_factor: 20
      final_div_factor: 100
      anneal_strategy: 'cos'

# Model architecture
scoring_function:
  hidden_dims: [64, 32]
  dropout: 0.2
  l2_lambda: 0.1
  activation:
    name: 'LeakyReLU'
    params:
      negative_slope: 0.1
  final_activation:
    name: 'Softplus'
    params:
      beta: 10  # Increased back to 10 for sharper separation

# Training constraints
tau:
  min: 0.1
  max: 0.9
  window_size: 5  # Increased from 0 for more stable tau

# Set size constraints
set_size:
  target: 1
  max: 3.0
  margin: 2