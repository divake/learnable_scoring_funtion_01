# Kinetics-400 specific configuration
inherit: base_config.yaml

# Device configuration
device: 1  # Use first GPU (cuda:0)
multi_gpu: true  # Enable multi-GPU training with DataParallel

dataset:
  name: kinetics400
  num_classes: 400
  feature_dir: "/mnt/ssd1/divake/vivit_kinetics400/kinetics400_features_extended"
  feature_dim: 768  # VideoMAE feature dimension
  samples_per_class: 10  # Now we have 10 samples per class

# Paths
plot_dir: plots/kinetics400_videomae
log_dir: logs/kinetics400_videomae

# Training parameters - adjusted for larger dataset
num_epochs: 150  # Increased from 100 to 150 for better convergence with more data
batch_size: 128  # Increased from 64 to 128 since we have more data
learning_rate: 0.0003  # Slightly reduced for more stable training with larger batches
lambda1: 1.5  # Coverage loss weight
lambda2: 1.0  # Set size loss weight
target_coverage: 0.9

# Training configuration
training:
  grad_clip:
    enabled: true
    max_norm: 1.0
  loss_weights:
    coverage: 1.5
    size: 1.0
    margin: 1

# Optimizer configuration
optimizer:
  name: 'AdamW'
  params:
    lr: 0.0003  # Adjusted to match learning_rate above
    weight_decay: 0.005
  scheduler:
    name: 'OneCycleLR'
    params:
      max_lr: 0.001
      pct_start: 0.3
      div_factor: 10
      final_div_factor: 50
      anneal_strategy: 'cos'

# Model architecture - adjusted for VideoMAE features and larger dataset
scoring_function:
  hidden_dims: [768, 512, 256]  # Increased capacity for better learning with more data
  dropout: 0.3  # Increased from 0.2 to 0.3 to prevent overfitting with more data
  l2_lambda: 0.01  # Increased from 0.05 to 0.01 for better regularization
  activation:
    name: 'LeakyReLU'
    params:
      negative_slope: 0.1
  final_activation:
    name: 'Softplus'
    params:
      beta: 5

# Training constraints
tau:
  min: 0.05
  max: 0.95
  window_size: 10

# Set size constraints
set_size:
  target: 1.5
  max: 5.0
  margin: 1 