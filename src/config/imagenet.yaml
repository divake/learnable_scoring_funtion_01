# ImageNet specific configuration
inherit: base_config.yaml

# Device configuration
device: 0

# Caching configuration
cache:
  enabled: true
  dir: cache  # Will be created under base_dir
  compression: false  # Whether to use compression for cached files

dataset:
  name: imagenet
  num_classes: 1000
  input_size: [224, 224]
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  # Dataset splits:
  # - Train: 30,000 samples
  # - Calibration: 10,000 samples
  # - Test: 10,000 samples

model:
  architecture: vit_base_patch16_224
  pretrained_path: models/vit_imagenet.pth
  img_size: 224
  drop_path_rate: 0.1
  drop_rate: 0.1

# Paths
plot_dir: plots/imagenet
log_dir: logs/imagenet
data_dir: data/imagenet

# Training parameters
num_epochs: 50  # Reduced from 50 for faster experimentation
batch_size: 256  # Reduced from 512 for better stability
target_coverage: 0.9

# Training data subset configuration
# Set to 1.0 for full dataset, 0.3 for 30% of data
training_subset_fraction: 1.0  # Using 30% of training data for faster experimentation

# Training configuration
training:
  grad_clip:
    enabled: true
    max_norm: 0.5
  loss_weights:
    coverage: 5.0   # lambda1 - reduced from 10.0 for better balance
    size: 2.0      # lambda2 - increased to better control set size
    margin: 0.05   # margin loss weight - further reduced

# Optimizer configuration
optimizer:
  name: 'AdamW'
  params:
    lr: 0.00005  # Much lower base learning rate for stability
    weight_decay: 0.01  # Increased like CIFAR-100
  scheduler:
    name: 'OneCycleLR'
    params:
      max_lr: 0.00005  # Much lower max LR
      pct_start: 0.2  # More gradual warmup like CIFAR-100
      div_factor: 20  # Start at lr/20
      final_div_factor: 100  # End at lr/100
      anneal_strategy: 'cos'

# Model architecture
scoring_function:
  hidden_dims: [256, 128]  # Simplified network
  dropout: 0.2  # Match CIFAR-100
  l2_lambda: 0.001  # Reduced L2 regularization for better learning
  activation:
    name: 'LeakyReLU'
    params:
      negative_slope: 0.1
  final_activation:
    name: 'None'  # For proper 0-1 range
    params: {}

# Training constraints
tau:
  min: 0.1
  max: 0.9
  window_size: 5

# Set size constraints
set_size:
  target: 1
  max: 10.0  # More flexible for 1000 classes
  margin: 0.1  # Reduced margin to avoid huge losses

# Training dynamics
training_dynamics:
  stability_factor: 0.001  # Reduced for less regularization
  separation_factor: 2.0   # Much reduced from 20.0 - was too aggressive
  perturbation_noise: 0.005  # Reduced noise
  permutation_augmentation_prob: 0.3  # Reduced permutation frequency
  coverage_tolerance: 0.02
  coverage_closeness_threshold: 0.01
  coverage_deficit_threshold: 0.02
  size_penalty_boost: 1.5
  coverage_boost: 2.0
  xavier_init_gain: 1.0
  num_workers: 2
  ece_test_points: 10 