# Common parameters for all scoring functions
device: 1  # GPU=0/1 and CPU=-1
target_coverage: 0.9

# Directory paths
data_dir: data  # Directory where datasets are stored
base_dir: .     # Base directory for relative paths
plot_dir: plots/conformal
log_dir: logs/conformal
result_dir: results/conformal

# Default dataset configuration
dataset:
  name: cifar10  # Default dataset
  num_classes: 10
  input_size: [32, 32]
  mean: [0.4914, 0.4822, 0.4465]
  std: [0.2023, 0.1994, 0.2010]
  batch_size: 128
  num_workers: 4

# Dataset-specific configurations
cifar10:
  data_dir: data/cifar10
  # Any other CIFAR-10 specific parameters

cifar100:
  num_classes: 100
  input_size: [32, 32]
  mean: [0.5071, 0.4865, 0.4409]
  std: [0.2673, 0.2564, 0.2762]
  data_dir: data/cifar100

imagenet:
  num_classes: 1000
  input_size: [224, 224]
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  data_dir: data/imagenet

vlm:
  num_classes: 4  # A, B, C, D options for multiple choice
  data_dir: /ssd_4TB/divake/learnable_scoring_funtion_01/data/vlm  # Absolute path to VLM data
  default_model: cogagent-vqa-hf  # Default VLM model
  default_dataset: ai2d  # Default VLM dataset
  split_ratio: [0.7, 0.1, 0.2]  # Train/cal/test split ratio
  # VLM-specific adjustments for conformal prediction
  apply_softmax: true  # Apply softmax to convert raw logits to probabilities
  use_first_n_logits: 4  # Use only the first 4 logits (for A, B, C, D)
  batch_size: 128  # Batch size for dataloaders
  num_workers: 4  # Number of workers for dataloaders
  # Run all models flag
  run_all_models: true  # Set to true to run analysis on all models
  # List of available VLM models
  models: [
    "Yi-VL-6B", 
    "Qwen-VL-Chat", 
    "mplug-owl2-llama2-7b", 
    "Monkey-Chat", 
    "Monkey", 
    "MoE-LLaVA-Phi2-2.7B-4e", 
    "llava-v1.6-vicuna-7b", 
    "llava-v1.6-vicuna-13b", 
    "llava-v1.6-34b", 
    "internlm-xcomposer2-vl-7b", 
    "cogagent-vqa-hf"
  ]
  # List of available VLM datasets
  datasets: ["ai2d", "mmbench", "oodcv", "scienceqa", "seedbench"]

# Model configurations
model:
  architecture: resnet18  # Default for CIFAR-10
  pretrained_path: models/resnet18_cifar10_best.pth
  img_size: 32  # Default for CIFAR datasets
  drop_path_rate: 0.0  # Default value
  drop_rate: 0.0  # Default value

# Model paths for different datasets
model_paths:
  cifar10: models/resnet18_cifar10_best.pth
  cifar100: models/vit_cifar100.pth
  imagenet: models/vit_imagenet.pth
  vlm: null  # No model path needed for VLM (using precomputed logits)

# Model-specific configurations
model_configs:
  cifar10: {}
  cifar100:
    architecture: vit_base_patch16_224_in21k
    img_size: 96
    drop_path_rate: 0.1
    drop_rate: 0.1
  imagenet:
    architecture: vit_base_patch16_224
    img_size: 224
    drop_path_rate: 0.1
    drop_rate: 0.1
  vlm:
    # VLM uses a dummy model since we're working with precomputed logits
    use_dummy_model: true
    # Conformal prediction settings for VLM
    logit_processing: "softmax"  # How to process logits: "softmax", "normalize", or "raw"

# Batch size for evaluation
batch_size: 128

# Scoring function specific parameters
scoring_functions:
  # 1-p specific parameters (if any)
  1-p: {}
  
  # APS specific parameters
  APS: {}
  
  # LogMargin specific parameters
  LogMargin: {}
  
  # Sparsemax specific parameters
  Sparsemax:
    delta: 0.15  # Threshold parameter for controlling sparsity

# Empty prediction set handling
# Options: 
#   - 'keep_empty': Keep empty sets as is (most honest, 0% coverage for those samples)
#   - 'add_most_likely': Add the most likely class (maintains some coverage)
#   - 'add_top_k': Add top-k most likely classes (specify k with empty_set_top_k)
empty_set_handling: 'keep_empty'
empty_set_top_k: 3  # Only used when empty_set_handling is 'add_top_k'

# Logging settings
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s' 